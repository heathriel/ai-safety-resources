# Bias Detection and Mitigation Demo

"""
This notebook demonstrates how to use AI Fairness 360 to detect, visualize, and mitigate biases in AI models using real-world datasets.
"""

# Step 1: Install AI Fairness 360
# Uncomment and run this line if AIF360 is not already installed in your environment
# !pip install aif360

# Step 2: Import Required Libraries
import pandas as pd
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Step 3: Load and Prepare Dataset
# Replace 'dataset.csv' with the path to your dataset
data = pd.read_csv('dataset.csv')

# Convert to BinaryLabelDataset format
dataset = BinaryLabelDataset(
    df=data,
    label_names=['outcome'],  # Replace with the actual name of your label column
    protected_attribute_names=['gender']  # Replace with the actual protected attribute
)

# Step 4: Detect Bias
# Define privileged and unprivileged groups
privileged_groups = [{'gender': 1}]  # Replace '1' with the value representing the privileged group
unprivileged_groups = [{'gender': 0}]  # Replace '0' with the value representing the unprivileged group

# Calculate bias metrics
metric = BinaryLabelDatasetMetric(dataset, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)
print("Disparate Impact:", metric.disparate_impact())

# Step 5: Visualize Bias
privileged = dataset.subset(lambda x: x['gender'] == 1)
unprivileged = dataset.subset(lambda x: x['gender'] == 0)

plt.hist(
    [privileged.labels, unprivileged.labels],
    label=['Privileged', 'Unprivileged'],
    bins=2
)
plt.xlabel('Outcome')
plt.ylabel('Frequency')
plt.title('Outcome Distribution by Protected Attribute')
plt.legend()
plt.show()

# Step 6: Mitigate Bias with Reweighing
rw = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)
dataset_transf = rw.fit_transform(dataset)

# Step 7: Evaluate Model Performance (Before and After Mitigation)
# Split features and labels
X_train, y_train = dataset.features, dataset.labels.ravel()
X_train_transf, y_train_transf = dataset_transf.features, dataset_transf.labels.ravel()

# Train Logistic Regression Model on Original Data
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_train)
print("Accuracy before mitigation:", accuracy_score(y_train, y_pred))

# Train Logistic Regression Model on Transformed (Mitigated) Data
model.fit(X_train_transf, y_train_transf)
y_pred_transf = model.predict(X_train_transf)
print("Accuracy after mitigation:", accuracy_score(y_train_transf, y_pred_transf))